{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test=None):\n",
    "    train_data = np.loadtxt(\"train_image.csv\", delimiter=\",\")\n",
    "    test_data = np.loadtxt(\"test_image.csv\",delimiter=\",\")\n",
    "    train_labels = np.loadtxt(\"train_label.csv\")\n",
    "    if test is None:\n",
    "        return train_data, test_data, train_labels\n",
    "    test_labels = np.loadtxt(\"test_label.csv\")\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "        \n",
    "\n",
    "def lab_2_onehot(labels):\n",
    "    vecs = 0.01*np.ones((labels.shape[0], 10))\n",
    "    for i in range(labels.shape[0]):\n",
    "        vecs[i][int(labels[i])] = 0.99\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_data(test=True)\n",
    "Y_train_one_hot = lab_2_onehot(Y_train)\n",
    "Y_test_one_hot  = lab_2_onehot(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 50\n",
    "nx = 784\n",
    "nh = 64\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "params = {\"W1\": np.random.randn(nh, nx)/np.sqrt(nx),\n",
    "          \"b1\": np.ones((nh, 1))/np.sqrt(nx),\n",
    "          \"W2\": np.random.randn(10,nh)/np.sqrt(nh),\n",
    "          \"b2\": np.ones((10,1))/np.sqrt(nh)\n",
    "         }\n",
    "batches = 60000//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-88fd41fb5961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mY_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# get mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batches' is not defined"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid activation function.\n",
    "\n",
    "    inputs: z\n",
    "    outputs: sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def compute_loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    compute loss function\n",
    "    \"\"\"\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "def feed_forward(X, params):\n",
    "    \"\"\"\n",
    "    feed forward network: 2 - layer neural net\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "\n",
    "    return:\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "\n",
    "    # Z1 = W1.dot(x) + b1\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "\n",
    "    # A1 = sigmoid(Z1)\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "\n",
    "    # Z2 = W2.dot(A1) + b2\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "\n",
    "    # A2 = softmax(Z2)\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def back_propagate(X, Y, params, cache, m_batch):\n",
    "    \"\"\"\n",
    "    back propagation\n",
    "\n",
    "    inputs:\n",
    "        params: dictionay a dictionary contains all the weights and biases\n",
    "        cache: dictionay a dictionary contains all the fully connected units and activations\n",
    "\n",
    "    return:\n",
    "        grads: dictionay a dictionary contains the gradients of corresponding weights and biases\n",
    "    \"\"\"\n",
    "    # error at last layer\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "\n",
    "    # gradients at last layer (Py2 need 1. to transform to float)\n",
    "    dW2 = (1. / m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1. / m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    # back propgate through first layer\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "\n",
    "    # gradients at first layer (Py2 need 1. to transform to float)\n",
    "    dW1 = (1. / m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1. / m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    # shuffle training set\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        # get mini-batch\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        # forward and backward\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache, m_batch)\n",
    "\n",
    "        # with momentum (optional)\n",
    "        dW1 = (momentum * dW1 + (1. - momentum) * grads[\"dW1\"])\n",
    "        db1 = (momentum * db1 + (1. - momentum) * grads[\"db1\"])\n",
    "        dW2 = (momentum * dW2 + (1. - momentum) * grads[\"dW2\"])\n",
    "        db2 = (momentum * db2 + (1. - momentum) * grads[\"db2\"])\n",
    "\n",
    "        # gradient descent\n",
    "        params[\"W1\"] = params[\"W1\"] - lr * dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - lr * db1\n",
    "        params[\"W2\"] = params[\"W2\"] - lr * dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - lr * db2\n",
    "\n",
    "    # forward pass on training set\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_loss = compute_loss(Y_train, cache[\"A2\"])\n",
    "\n",
    "    # forward pass on test set\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_loss = compute_loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training loss = {}, test loss = {}\".format(\n",
    "        i + 1, train_loss, test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
